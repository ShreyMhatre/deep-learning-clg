{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGuGsIsJ3meMw/OyHqvQ3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyMhatre/deep-learning-clg/blob/main/DL_EXP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLShmOwvUSg2",
        "outputId": "41ca1226-e930-4be3-e098-9e9667e80e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SGD:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8843 - loss: 0.3879 \n",
            "\n",
            "Mini Batch:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3620 - loss: 1.1155 \n",
            "\n",
            "Momentum:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8042 - loss: 0.5681 \n",
            "\n",
            "Nesterov:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6736 - loss: 0.6170 \n",
            "\n",
            "Adagrad:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3264 - loss: 1.3392 \n",
            "\n",
            "Adam:\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6484 - loss: 0.7243 \n",
            "+------------------------------+----------+-------+\n",
            "|          Optimizer           | Accuracy |  Time |\n",
            "+------------------------------+----------+-------+\n",
            "| Stochastic Gradient Descent  |  88.89%  | 6.77s |\n",
            "| Mini Batch Gradient Descent  |  35.56%  | 1.79s |\n",
            "|         Momentum GD          |  80.0%   | 1.88s |\n",
            "|         Nestorev GD          |  66.67%  | 1.89s |\n",
            "|          Adagrad GD          |  33.33%  |  2.9s |\n",
            "|       Adam Learning GD       |  64.44%  | 2.28s |\n",
            "+------------------------------+----------+-------+\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import time\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Load and preprocess\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "y = enc.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 30% test data\n",
        "\n",
        "def create_model():\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, activation='relu', input_shape=(4,)),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "lr=0.001\n",
        "batch_size=32\n",
        "epoch=25\n",
        "momentum=0.9\n",
        "\n",
        "sgd_model = create_model()\n",
        "sgd_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "mini_batch_model = create_model()\n",
        "mini_batch_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "momentum_model = create_model()\n",
        "momentum_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "nesterov_model = create_model()\n",
        "nesterov_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "adagrad_model = create_model()\n",
        "adagrad_model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr) , loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "adam_model = create_model()\n",
        "adam_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nSGD:\")\n",
        "start_sgd = time.time()\n",
        "sgd_model.fit(X_train, y_train, epochs=epoch, verbose=0, batch_size=1)\n",
        "end_sgd = time.time()\n",
        "loss, sgd_acc = sgd_model.evaluate(X_test, y_test)\n",
        "sgd_time = round(end_sgd - start_sgd, 2)\n",
        "\n",
        "print(\"\\nMini Batch:\")\n",
        "start_mini_batch = time.time()\n",
        "mini_batch_model.fit(X_train, y_train, epochs=epoch, verbose=0, batch_size=batch_size)\n",
        "end_mini_batch = time.time()\n",
        "loss, mini_batch_acc = mini_batch_model.evaluate(X_test, y_test)\n",
        "mini_batch_time = round(end_mini_batch - start_mini_batch, 2)\n",
        "\n",
        "print(\"\\nMomentum:\")\n",
        "start_momentum = time.time()\n",
        "momentum_model.fit(X_train, y_train, epochs=epoch, verbose=0)\n",
        "end_momentum = time.time()\n",
        "loss, momentum_acc = momentum_model.evaluate(X_test, y_test)\n",
        "momentum_time = round(end_momentum - start_momentum, 2)\n",
        "\n",
        "print(\"\\nNesterov:\")\n",
        "start_nesterov = time.time()\n",
        "nesterov_model.fit(X_train, y_train, epochs=epoch, verbose=0)\n",
        "end_nesterov = time.time()\n",
        "loss, nesterov_acc = nesterov_model.evaluate(X_test, y_test)\n",
        "nesterov_time = round(end_nesterov - start_nesterov, 2)\n",
        "\n",
        "print(\"\\nAdagrad:\")\n",
        "start_adagrad = time.time()\n",
        "adagrad_model.fit(X_train, y_train, epochs=epoch, verbose=0)\n",
        "end_adagrad = time.time()\n",
        "loss, adagrad_acc = adagrad_model.evaluate(X_test, y_test)\n",
        "adagrad_time = round(end_adagrad - start_adagrad, 2)\n",
        "\n",
        "print(\"\\nAdam:\")\n",
        "start_adam = time.time()\n",
        "adam_model.fit(X_train, y_train, epochs=epoch, verbose=0)\n",
        "end_adam = time.time()\n",
        "loss, adam_acc = adam_model.evaluate(X_test, y_test)\n",
        "adam_time = round(end_adam - start_adam, 2)\n",
        "\n",
        "\n",
        "my_table = PrettyTable()\n",
        "my_table.field_names = [\"Optimizer\", \"Accuracy\", \"Time\"]\n",
        "my_table.add_row([\"Stochastic Gradient Descent \", f\"{round(sgd_acc*100, 2)}%\", f\"{sgd_time}s\"])\n",
        "my_table.add_row([\"Mini Batch Gradient Descent \", f\"{round(mini_batch_acc*100, 2)}%\", f\"{mini_batch_time}s\"])\n",
        "my_table.add_row([\"Momentum GD\", f\"{round(momentum_acc*100, 2)}%\", f\"{momentum_time}s\"])\n",
        "my_table.add_row([\"Nestorev GD\", f\"{round(nesterov_acc*100, 2)}%\", f\"{nesterov_time}s\"])\n",
        "my_table.add_row([\"Adagrad GD\", f\"{round(adagrad_acc*100, 2)}%\", f\"{adagrad_time}s\"])\n",
        "my_table.add_row([\"Adam Learning GD\", f\"{round(adam_acc*100, 2)}%\", f\"{adam_time}s\"])\n",
        "\n",
        "print(my_table)"
      ]
    }
  ]
}